[{"grant_id":"US20170011179A1","inventor":"Jawad Ali ArshadRaheel ImtiazWaseem UllahMarghub Alam Mirza","claims":"What is claimed is:\n1. A method of providing a plurality of virtual immediate waiting rooms, the method comprising:\nproviding on a server computer, a plurality of software queues, each queue associated with one of a plurality of different legal jurisdictions a patient may be located in;\ngenerating a graphical user interface that when rendered on a computing device displays at least an option for a patient to request an immediate encounter with a healthcare provider;\ndetermining a legal jurisdiction associated with the patent;\nplacing the patient into a queue corresponding to the legal jurisdiction associated therewith;\ngranting access to a queue to those healthcare providers licensed in the legal jurisdiction associated therewith;\nenabling a healthcare provider to select a patient from one or more queues they have access to and sending a notification thereof to the patient; and\nestablishing an encounter between the healthcare provider and the selected patient.\n2. The method according to claim 1, wherein the plurality of software queues is maintained utilizing one or more databases in communication with the server computer.\n3. The method according to claim 1, wherein legal jurisdictions represent different states in the United States of America.\n4. The method according to claim 1, wherein said encounter between the selected patient and their associated healthcare provider consists of at least one of a voice call, video call, and text session.\n5. The method according to claim 1, further comprising dispatching a healthcare practitioner to a patient location.\n6. A method of providing a plurality of virtual scheduled waiting rooms, the method comprising:\nproviding on a server computer, a plurality of software queues, each queue associated with one of a plurality of healthcare providers;\nreceiving a request from a patient on a computing device to schedule an encounter with a health care provider;\ngenerating a graphical user interface that when rendered on a computing device displays an option for a patient to select from among a list of healthcare providers that meet one or more criteria;\nenabling a patient to select one of the healthcare providers on the list;\nsending a request to the server for an encounter between the patient and the selected healthcare provider at a desired date and time;\ncreating an appointment for the patient with the selected healthcare provider at the desired data and time;\nplacing the patient in the queue corresponding to the selected healthcare provider a predetermined time prior to the appointment;\nsending a notification to the patient when the selected healthcare provider is ready for the encounter; and\nestablishing an encounter between the selected healthcare provider and the patient.\n7. The method according to claim 6, wherein only healthcare providers in the same legal jurisdiction as the patient are provided on the list.\n8. The method according to claim 6, wherein the criteria used to create the list consists of at least one of specialty type, name, location, fee, hospital affiliation, medical group membership, insurance accepted, legal jurisdiction and availability on certain dates and times.\n9. The method according to claim 6, further comprising sending a notification to the selected healthcare provider when the patient is ready for the encounter.\n10. The method according to claim 6, wherein the plurality of software queues is maintained utilizing one or more databases in communication with the server computer.\n11. The method according to claim 6, wherein legal jurisdictions represent different states in the United States of America.\n12. The method according to claim 6, wherein said encounter between the selected patient and their associated healthcare provider consists of at least one of a voice call, video call, and text session.\n13. The method according to claim 6, further comprising dispatching a healthcare practitioner to a patient location.\n14. A method of providing a plurality of virtual scheduled waiting rooms for use on a computing device, the method comprising:\ngenerating a graphical user interface that when rendered on the computing device displays an option for a patient to schedule an appointment for an encounter with a healthcare provider;\nreceiving a request via the graphical user interface to schedule an encounter with a health care provider;\ngenerating a graphical user interface that when rendered on a computing device enables the patient to enter one or more criteria for selecting a healthcare provider;\nutilizing the criteria entered by the patient, querying a healthcare provider database located on a server computer accessible over the internet, wherein the server computer comprises a plurality of software queues, each queue associated with one of a plurality of healthcare providers;\nreceiving in response from the server computer, a list of healthcare providers that satisfy the one or more criteria;\ngenerating a graphical user interface that when rendered on a computing device displays the list of healthcare providers;\ngenerating a graphical user interface that when rendered on a computing device enables the patient to select a healthcare provider on the list;\nsending a request to the server computer for an encounter between the patient and the selected healthcare provider at a desired date and time, and in response, the server computer creating an appointment for the patient with the selected healthcare provider at the desired data and time and placing the patient in the queue corresponding to the selected healthcare provider a predetermined time prior to the appointment;\nnotifying the patient when the selected healthcare provider is ready for the encounter; and\nestablishing an encounter between the selected healthcare provider and the patient.\n15. The method according to claim 14, wherein only healthcare providers in the same legal jurisdiction as the patient are provided on the list.\n16. The method according to claim 14, wherein the criteria used to create the list consists of at least one of specialty type, name, location, fee, hospital affiliation, medical group membership, insurance accepted, legal jurisdiction and availability on certain dates and times.\n17. The method according to claim 14, further comprising sending a notification to the selected healthcare provider when the patient is ready for the encounter.\n18. The method according to claim 14, wherein the plurality of software queues is maintained utilizing one or more databases in communication with the server computer.\n19. The method according to claim 14, wherein said encounter between the selected patient and their associated healthcare provider consists of at least one of a voice call, video call, and text session.\n20. The method according to claim 14, further comprising dispatching a healthcare practitioner to a patient location.\n","abstract":"A telemedicine system including a care coordination software platform allows for patient monitoring at home and connects patients to their medical teams via telemedicine using a HIPAA compliant video portal augmented by remote assisted physical examination, performance of diagnostic testing including labs and x-rays, and provision of appropriate treatment and prescriptions. Medical care is provided at the patient's location without the patient having to travel or spend time in waiting rooms, provides treatment based on objective physical examination data and any appropriate diagnostic testing, and provides validation of patient identity. Healthcare providers are made available via online video encounters to communicate with patients. Allied healthcare workers are dispatched to be in physical proximity to the patient to assist in physical examination, and provide diagnostic data. Providers order appropriate treatments and prescriptions based on examination findings and diagnostics. The telemedicine system interfaces with medical sensors and collects data wired or wirelessly."},{"grant_id":"US20190087544A1","inventor":"Marcia Peterson","claims":"What is claimed is:\n1. An apparatus comprising:\na processor and a memory, the processor to configure the memory according to a digital twin of a first healthcare procedure, the digital twin including a data structure created from tasks defining the first healthcare procedure and a list of items to be used in the first healthcare procedure to model the tasks of the first healthcare procedure and items associated with each task of the first healthcare procedure, the digital twin arranged for query and simulation via the processor to model the first healthcare procedure for a first patient,\nthe digital twin to at least:\nreceive input regarding a first item at a first location;\ncompare the first item to the items associated with each task of the first healthcare procedure; and\nwhen the first item matches an item associated with a task of the first healthcare procedure, record the first item and approval for the first healthcare procedure and update the digital twin based on the first item; and\nwhen the first item does not match an item associated with a task of the first healthcare procedure, log the first item.\n2. The apparatus of claim 1, further including a sensor to identify the first item at the first location.\n3. The apparatus of claim 2, wherein the sensor is to verify whether the first item was used in the first healthcare procedure for the first patient.\n4. The apparatus of claim 3, wherein, when the first item was used in the first healthcare procedure for the first patient, a preference card is updated based on the first item.\n5. The apparatus of claim 2, wherein the sensor is incorporated into at least one of glasses or an eye shield, and wherein information regarding the first item is displayed via the at least one of glasses or eye shield.\n6. The apparatus of claim 2, wherein the sensor is incorporated into a cart with a computing device.\n7. The apparatus of claim 1, wherein the digital twin is periodically retrained and redeployed based on feedback including at least one of the update or the log.\n8. A computer-readable storage medium comprising instructions which, when executed by a processor, cause a machine to implement at least:\na digital twin of a first healthcare procedure, the digital twin including a data structure created from tasks defining the first healthcare procedure and a list of items to be used in the first healthcare procedure to model the tasks of the first healthcare procedure and items associated with each task of the first healthcare procedure, the digital twin arranged for query and simulation via the processor to model the first healthcare procedure for a first patient,\nthe digital twin to at least:\nreceive input regarding a first item at a first location;\ncompare the first item to the items associated with each task of the first healthcare procedure; and\nwhen the first item matches an item associated with a task of the first healthcare procedure, record the first item and approval for the first healthcare procedure and update the digital twin based on the first item; and\nwhen the first item does not match an item associated with a task of the first healthcare procedure, log the first item.\n9. The computer-readable storage medium of claim 8, wherein the digital twin is to interact with a sensor to identify the first item at the first location.\n10. The computer-readable storage medium of claim 9, wherein the sensor is to verify whether the first item was used in the first healthcare procedure for the first patient.\n11. The computer-readable storage medium of claim 10, wherein, when the first item was used in the first healthcare procedure for the first patient, a preference card is updated based on the first item.\n12. The computer-readable storage medium of claim 9, wherein the sensor is incorporated into at least one of glasses or an eye shield, and wherein information regarding the first item is displayed via the at least one of glasses or eye shield.\n13. The computer-readable storage medium of claim 9, wherein the sensor is incorporated into a cart with a computing device.\n14. The computer-readable storage medium of claim 8, wherein the digital twin is periodically retrained and redeployed based on feedback including at least one of the update or the log.\n15. A method comprising:\nreceiving, using a processor, input regarding a first item at a first location;\ncomparing, using the processor, the first item to items associated with each task of a first healthcare procedure, the items associated with each task of the first healthcare protocol modeled using a digital twin of the first healthcare protocol, the digital twin including a data structure created from tasks defining the first healthcare procedure and a list of items to be used in the first healthcare procedure to model the tasks of the first healthcare procedure and items associated with each task of the first healthcare procedure, the digital twin arranged for query and simulation via the processor to model the first healthcare procedure for a first patient;\nwhen the first item matches an item associated with a task of the first healthcare procedure, recording the first item and approval for the first healthcare procedure and update the digital twin based on the first item; and\nwhen the first item does not match an item associated with a task of the first healthcare procedure, logging the first item.\n16. The method of claim 15, wherein the digital twin is to interact with a sensor to identify the first item at the first location.\n17. The method of claim 16, wherein the sensor is to verify whether the first item was used in the first healthcare procedure for the first patient.\n18. The method of claim 17, wherein, when the first item was used in the first healthcare procedure for the first patient, the method further includes updating a preference card based on the first item.\n19. The method of claim 15, wherein the sensor is incorporated into at least one of glasses, an eye shield, or a cart, and wherein information regarding the first item is displayed via the at least one of glasses, eye shield, or cart.\n20. The method of claim 15, further including periodically retraining and redeploying the digital twin based on feedback including at least one of the updating or the logging.\n","abstract":""},{"grant_id":"US11404148B2","inventor":"Donald E. OwenMehmet Mert ÖzGarret N. Erskine","claims":"What is claimed is:\n1. A computer-implemented method for compartmentalizing a virtual assistant, executed on a computing device, comprising:\nobtaining encounter information via a compartmentalized virtual assistant during a patient encounter, wherein the compartmentalized virtual assistant includes a core functionality module that verbally interacts with an encounter participant of the patient encounter, wherein the core functionality module verbally interacts with at least one of a patient and a medical professional as the encounter participant during a portion of the patient encounter;\ndetermining, by the compartmentalized virtual assistant, that one of a medication and a medical procedure was mentioned by at least one of the patient and the medical professional while verbally interacting with the virtual assistant during the portion of the patient encounter;\ndownloading and activating one or more additional functionalities to the compartmentalized virtual assistant during the patient encounter based upon, at least in part, the compartmentalized virtual assistant determining that one of the medication and the medical procedure is mentioned during the portion of the patient encounter, wherein at least one of the one or more additional functionalities added and activated to the compartmentalized virtual assistant during the patient encounter includes a medical coverage datasource interaction functionality to determine whether one of the medication and the medical procedure mentioned is covered by a medical insurance plan of the patient, wherein the medical coverage datasource interaction functionality has not been downloaded and activated prior to the compartmentalized virtual assistant determining that one of the medication and the medical procedure is mentioned, and wherein the medical coverage datasource interaction functionality is downloaded and activated after the compartmentalized virtual assistant determines that one of the medication and the medical procedure is mentioned; and\nproviding feedback to the second encounter participant from the compartmentalized virtual assistant based upon the one or more additional functionalities to inform the second encounter participant whether one of the medication and the medical procedure mentioned is covered by the medical insurance plan of the patient.\n2. The computer-implemented method of claim 1 wherein downloading one or more additional functionalities to the compartmentalized virtual assistant on an as-needed basis includes:\nloading one or more additional functionality modules for the compartmentalized virtual assistant when needed to effectuate the additional functionalities.\n3. The computer-implemented method of claim 1 further comprising:\nremoving one or more existing functionalities from the compartmentalized virtual assistant on an as-needed basis.\n4. The computer-implemented method of claim 3 wherein removing one or more existing functionalities from the compartmentalized virtual assistant on an as-needed basis includes:\nunloading one or more existing functionality modules of the compartmentalized virtual assistant when no longer needed to effectuate the existing functionalities.\n5. The computer-implemented method of claim 1 further comprising:\nprocessing the encounter information to generate an encounter transcript.\n6. The computer-implemented method of claim 5 further comprising:\nprocessing at least a portion of the encounter transcript to populate at least a portion of a medical record associated with the patient encounter.\n7. The computer-implemented method of claim 1 wherein obtaining encounter information via a compartmentalized virtual assistant during a patient encounter includes:\naudibly obtaining encounter information via the compartmentalized virtual assistant during the patient encounter.\n8. A computer program product residing on a non-transitory computer readable medium having a plurality of instructions stored thereon which, when executed by a processor, cause the processor to perform operations comprising:\nobtaining encounter information via a compartmentalized virtual assistant during a patient encounter, wherein the compartmentalized virtual assistant includes a core functionality module that verbally interacts with an encounter participant of the patient encounter, wherein the core functionality module verbally interacts with at least one of a patient and a medical professional as the encounter participant during a portion of the patient encounter;\ndetermining, by the compartmentalized virtual assistant, that one of a medication and a medical procedure was mentioned by at least one of the patient and the medical professional while verbally interacting with the virtual assistant during the portion of the patient encounter;\ndownloading and activating one or more additional functionalities to the compartmentalized virtual assistant during the patient encounter based upon, at least in part, the compartmentalized virtual assistant determining that one of the medication and the medical procedure is mentioned during the portion of the patient encounter, wherein at least one of the one or more additional functionalities added and activated to the compartmentalized virtual assistant during the patient encounter includes a medical coverage datasource interaction functionality to determine whether one of the medication and the medical procedure mentioned is covered by a medical insurance plan of the patient, wherein the medical coverage datasource interaction functionality has not been downloaded and activated prior to the compartmentalized virtual assistant determining that one of the medication and the medical procedure is mentioned, and wherein the medical coverage datasource interaction functionality is downloaded and activated after the compartmentalized virtual assistant determines that one of the medication and the medical procedure is mentioned; and\nproviding feedback to the second encounter participant from the compartmentalized virtual assistant based upon the one or more additional functionalities to inform the second encounter participant whether one of the medication and the medical procedure mentioned is covered by the medical insurance plan of the patient.\n9. The computer program product of claim 8 wherein downloading one or more additional functionalities to the compartmentalized virtual assistant on an as-needed basis includes:\nloading one or more additional functionality modules for the compartmentalized virtual assistant when needed to effectuate the additional functionalities.\n10. The computer program product of claim 8 further comprising:\nremoving one or more existing functionalities from the compartmentalized virtual assistant on an as-needed basis.\n11. The computer program product of claim 10 wherein removing one or more existing functionalities from the compartmentalized virtual assistant on an as-needed basis includes:\nunloading one or more existing functionality modules of the compartmentalized virtual assistant when no longer needed to effectuate the existing functionalities.\n12. The computer program product of claim 8 further comprising:\nprocessing the encounter information to generate an encounter transcript.\n13. The computer program product of claim 12 further comprising:\nprocessing at least a portion of the encounter transcript to populate at least a portion of a medical record associated with the patient encounter.\n14. The computer program product of claim 8 wherein obtaining encounter information via a compartmentalized virtual assistant during a patient encounter includes:\naudibly obtaining encounter information via the compartmentalized virtual assistant during the patient encounter.\n15. A computing system including a processor and memory configured to perform operations comprising:\nobtaining encounter information via a compartmentalized virtual assistant during a patient encounter, wherein the compartmentalized virtual assistant includes a core functionality module that verbally interacts with an encounter participant of the patient encounter, wherein the core functionality module verbally interacts with at least one of a patient and a medical professional as the encounter participant during a portion of the patient encounter;\ndetermining, by the compartmentalized virtual assistant, that one of a medication and a medical procedure was mentioned by at least one of the patient and the medical professional while verbally interacting with the virtual assistant during the portion of the patient encounter;\ndownloading and activating one or more additional functionalities to the compartmentalized virtual assistant during the patient encounter based upon, at least in part, the compartmentalized virtual assistant determining that one of the medication and the medical procedure is mentioned during the portion of the patient encounter, wherein at least one of the one or more additional functionalities added and activated to the compartmentalized virtual assistant during the patient encounter includes a medical coverage datasource interaction functionality to determine whether one of the medication and the medical procedure mentioned is covered by a medical insurance plan of the patient, wherein the medical coverage datasource interaction functionality has not been downloaded and activated prior to the compartmentalized virtual assistant determining that one of the medication and the medical procedure is mentioned, and wherein the medical coverage datasource interaction functionality is downloaded and activated after the compartmentalized virtual assistant determines that one of the medication and the medical procedure is mentioned; and\nproviding feedback to the second encounter participant from the compartmentalized virtual assistant based upon the one or more additional functionalities to inform the second encounter participant whether one of the medication and the medical procedure mentioned is covered by the medical insurance plan of the patient.\n16. The computing system of claim 15 wherein downloading one or more additional functionalities to the compartmentalized virtual assistant on an as-needed basis includes:\nloading one or more additional functionality modules for the compartmentalized virtual assistant when needed to effectuate the additional functionalities.\n17. The computing system of claim 15 further comprising:\nremoving one or more existing functionalities from the compartmentalized virtual assistant on an as-needed basis.\n18. The computing system of claim 17 wherein removing one or more existing functionalities from the compartmentalized virtual assistant on an as-needed basis includes:\nunloading one or more existing functionality modules of the compartmentalized virtual assistant when no longer needed to effectuate the existing functionalities.\n19. The computing system of claim 15 further comprising:\nprocessing the encounter information to generate an encounter transcript.\n20. The computing system of claim 19 further comprising:\nprocessing at least a portion of the encounter transcript to populate at least a portion of a medical record associated with the patient encounter.\n21. The computing system of claim 15 wherein obtaining encounter information via a compartmentalized virtual assistant during a patient encounter includes:\naudibly obtaining encounter information via the compartmentalized virtual assistant during the patient encounter.\n","abstract":"A method, computer program product, and computing system for compartmentalizing a virtual assistant is executed on a computing device and includes obtaining encounter information via a compartmentalized virtual assistant during a patient encounter, wherein the compartmentalized virtual assistant includes a core functionality module. One or more additional functionalities are added to the compartmentalized virtual assistant on an as-needed basis."},{"grant_id":"US11250383B2","inventor":"Dushyant SharmaPatrick A. NAYLORUwe Helmut Jost","claims":"What is claimed is:\n1. A computer-implemented method executed on a computer comprising:\ndetermining a time delay between when an audio signal from an encounter participant is received at one or more microphones on a first audio detection system and when the audio signal from the encounter participant is received at one or more microphones on a second audio detection system, wherein the first and second audio detection systems are located within a monitored space, and wherein the first audio detection system includes a body worn audio detection system positioned within the monitored space and worn by the encounter participant, wherein the body worn audio detection system includes the one or more microphones of the first audio detection system;\nlocating the first audio detection system with respect to the second audio detection system within the monitored space based upon, at least in part, the time delay between when the audio signal from the encounter participant is received on the first audio detection system and when the audio signal from the encounter participant is received on the second audio detection system, wherein locating the first audio detection system with respect to the second audio detection system within the monitored space includes defining a linear distance between the first audio detection system worn by the encounter participant and the second audio detection system based, at least in part, upon the time delay; and\nprocessing the audio signal, wherein the audio signal includes a first audio encounter information and a second audio encounter information, wherein processing the audio signal includes canceling interference between the first audio encounter information and the second audio encounter information.\n2. The computer-implemented method of claim 1 wherein locating the first audio detection system with respect to the second audio detection system within the monitored space further includes:\ndefining an angular location for the first audio detection system with respect to the second audio detection system.\n3. The computer-implemented method of claim 2 wherein the second audio detection system includes:\na fixed-location audio detection system positioned within the monitored space.\n4. The computer-implemented method of claim 3 wherein the fixed-location audio detection system includes a microphone array.\n5. The computer-implemented method of claim 4 wherein defining an angular location for the first audio detection system with respect to the second audio detection system includes:\nforming at least one audio recording beam via the microphone array of the fixed location audio detection system;\nsteering the at least one audio recording beam toward the first audio detection system; and\ndetermining the angle of the at least one audio recording beam with respect to the fixed location audio detection system.\n6. A computer program product residing on a non-transitory computer readable medium having a plurality of instructions stored thereon which, when executed by a processor, cause the processor to perform operations comprising:\ndetermining a time delay between when an audio signal from an encounter participant is received at one or more microphones on a first audio detection system and when the audio signal from the encounter participant is received at one or more microphones on a second audio detection system, wherein the first and second audio detection systems are located within a monitored space, and wherein the first audio detection system includes a body worn audio detection system positioned within the monitored space and worn by the encounter participant, wherein the body worn audio detection system includes the one or more microphones of the first audio detection system;\nlocating the first audio detection system with respect to the second audio detection system within the monitored space based upon, at least in part, the time delay between when the audio signal from the encounter participant is received on the first audio detection system and when the audio signal from the encounter participant is received on the second audio detection system, wherein locating the first audio detection system with respect to the second audio detection system within the monitored space includes defining a linear distance between the first audio detection system worn by the encounter participant and the second audio detection system based, at least in part, upon the time delay; and\nprocessing the audio signal, wherein the audio signal includes a first audio encounter information and a second audio encounter information, wherein processing the audio signal includes canceling interference between the first audio encounter information and the second audio encounter information.\n7. The computer program product of claim 6 wherein locating the first audio detection system with respect to the second audio detection system within the monitored space further includes:\ndefining an angular location for the first audio detection system with respect to the second audio detection system.\n8. The computer program product of claim 7 wherein the second audio detection system includes:\na fixed-location audio detection system positioned within the monitored space.\n9. The computer program product of claim 8 wherein the fixed-location audio detection system includes a microphone array.\n10. The computer program product of claim 9 wherein defining an angular location for the first audio detection system with respect to the second audio detection system includes:\nforming at least one audio recording beam via the microphone array of the fixed location audio detection system;\nsteering the at least one audio recording beam toward the first audio detection system; and\ndetermining the angle of the at least one audio recording beam with respect to the fixed location audio detection system.\n11. A computing system including a processor and memory configured to perform operations comprising:\ndetermining a time delay between when an audio signal from an encounter participant is received at one or more microphones on a first audio detection system and when the audio signal from the encounter participant is received at one or more microphones on a second audio detection system, wherein the first and second audio detection systems are located within a monitored space, and wherein the first audio detection system includes a body worn audio detection system positioned within the monitored space and worn by the encounter participant, wherein the body worn audio detection system includes the one or more microphones of the first audio detection system;\nlocating the first audio detection system with respect to the second audio detection system within the monitored space based upon, at least in part, the time delay between when the audio signal from the encounter participant is received on the first audio detection system and when the audio signal from the encounter participant is received on the second audio detection system, wherein locating the first audio detection system with respect to the second audio detection system within the monitored space includes defining a linear distance between the first audio detection system worn by the encounter participant and the second audio detection system based, at least in part, upon the time delay; and\nprocessing the audio signal, wherein the audio signal includes a first audio encounter information and a second audio encounter information, wherein processing the audio signal includes canceling interference between the first audio encounter information and the second audio encounter information.\n12. The computing system of claim 11 wherein locating the first audio detection system with respect to the second audio detection system within the monitored space further includes:\ndefining an angular location for the first audio detection system with respect to the second audio detection system.\n13. The computing system of claim 12 wherein the second audio detection system includes:\na fixed-location audio detection system positioned within the monitored space.\n14. The computing system of claim 13 wherein the fixed-location audio detection system includes a microphone array.\n15. The computer-implemented method of claim 14 wherein defining an angular location for the first audio detection system with respect to the second audio detection system includes:\nforming at least one audio recording beam via the microphone array of the fixed location audio detection system;\nsteering the at least one audio recording beam toward the first audio detection system; and\ndetermining the angle of the at least one audio recording beam with respect to the fixed location audio detection system.\n16. The computer-implemented method of claim 1 wherein locating the first audio detection system with respect to the second audio detection system within the monitored space further includes using a polar coordinate with the linear distance between the first audio detection system worn by the encounter participant and the second audio detection system.\n17. The computer-implemented method of claim 1 wherein the second audio detection system includes one microphone having a known and a fixed location.\n18. The computer-implemented method of claim 17 wherein locating the first audio detection system with respect to the second audio detection system within the monitored space is based upon, at least in part, triangulation.\n","abstract":"A method, computer program product, and computing system for determining a time delay between a first audio signal received on a first audio detection system and a second audio signal received on a second audio detection system. The first and second audio detection systems are located within a monitored space. The first audio detection system is located with respect to the second audio detection system within the monitored space."},{"grant_id":"US8589177B2","inventor":"Mohamed Haq","claims":"What is claimed is:\n1. A method comprising:\nestablishing a virtual clinic on a network, the virtual clinic having working relationships with specialists;\nestablishing a real-time communication between a user and a particular one of the specialists via the virtual clinic;\nproviding user information to the particular one of the specialists via the virtual clinic;\nproviding instructions from the particular one of the specialists to the user via the virtual clinic to perform a test using diagnostic test equipment operatively coupled to a diagnostic center at a remote location away from the user's home and away from a dedicated treatment facility, and the diagnostic center being administered by an administrator who is not a licensed physician;\nproviding a test result to the particular one of the specialists via the virtual clinic; and\nreceiving and storing a diagnosis from the specialist.\n2. The method of claim 1, further comprising storing locations of a plurality of diagnostic centers, storing a list of diagnostic testing equipment present at each different diagnostic center, generating a list of diagnostic centers from the plurality of diagnostic centers based at least in part on proximity to the user, and providing the list of diagnostic centers to the user.\n3. The method of claim 2, wherein the list of diagnostic centers is also based at least in part on the request for the particular one of the specialists.\n4. The method of claim 1, further comprising establishing a real-time communication between the particular one of the specialists and a consultant via the virtual clinic during the real-time communication between the user and the particular one of the specialists.\n5. The method of claim 4, wherein the consultant does not have access to an identity of the user.\n6. The method of claim 1, further comprising providing access to the real-time communication between the user and the particular one of the specialists to a third-party.\n7. The method of claim 6, wherein the third-party is a trainee.\n8. The method of claim 6, wherein the third-party is a peer to the particular one of the specialists.\n9. The method of claim 1, wherein the particular one of the specialists is a physician and the user is a patient.\n10. The method of claim 1, further comprising receiving the user information and determining the particular one of the specialists based on the user information and specialist information.\n11. The method of claim 1, further comprising establishing a real-time communication between the particular one of the specialists and a supervisor via the virtual clinic during the real-time communication between the user and the particular one of the specialists.\n12. The method of claim 1, further comprising storing the communication between the user and the particular one of the specialists and enabling access to the stored communication to a third-party.\n13. A system comprising:\na specialist device for use by a specialist and configured to receive user information from a virtual clinic, provide instructions to a user via the virtual clinic to perform a test with diagnostic test equipment operatively coupled to a diagnostic center at a remote location away from the user's home and away from a dedicated treatment facility, receive a test result from the virtual clinic, and provide a diagnosis of the user via the virtual clinic;\na user device for use by the user and configured to provide a request for the specialist to the virtual clinic and provide the test result to the virtual clinic using the diagnostic test equipment; and\na virtual clinic device configured to provide specialist information to the user device and establish real-time communication between the specialist device and the user device.\n14. The system of claim 13, wherein the virtual device is further configured to store locations of a plurality of diagnostic centers, to store a list of diagnostic testing equipment present at each different diagnostic center, to generate a list of diagnostic centers from the plurality of diagnostic centers based at least in part on proximity to the user, and to provide the list of diagnostic centers to the user device.\n15. The system of claim 14, wherein the list of diagnostic centers is generated based at least in part on the request for the specialist.\n16. The system of claim 13, wherein the virtual clinic device is further configured to establish a real-time communication between the specialist and a consultant via the virtual clinic during the real-time communication between the user and the specialist.\n17. The system of claim 16, wherein the virtual device is further configured to protect the anonymity of the user from the consultant.\n18. The system of claim 13, wherein the virtual clinic device is further configured to provide access to the real-time communication between the user and the specialist to a third-party.\n19. The system of claim 18, wherein the third-party is a trainee.\n20. The system of claim 18, wherein the third-party is a peer.\n21. The system of claim 13, wherein the specialist is a physician and the user is a patient.\n22. The system of claim 13, wherein the virtual clinic device is further configured to receive the user information from the user device, to determine the specialist based on the user information and the specialist information, and to provide at least some specialist information to the user device based on the determination.\n23. The system of claim 13, wherein the virtual clinic device is further configured to establish a real-time communication between the specialist and a supervisor via the virtual clinic during the real-time communication between the user and the specialist.\n24. The system of claim 13, wherein the virtual clinic device is further configured to store the communication between the user and the specialist and enable access to the stored communication to a third-party.\n25. A method comprising:\nestablishing a virtual clinic on a network, the virtual clinic having working relationships with specialists;\nestablishing real-time communication between an administrator who is not a licensed physician at a diagnostic center and a particular one of the specialists via the virtual clinic, the diagnostic center being at a remote location away from a user's home and away from a dedicated treatment facility;\nreceiving instructions to perform a test with diagnostic test equipment operatively coupled with the diagnostic center, the instructions received from the particular one of the specialists; and\nproviding a test result from the diagnostic test equipment to the specialist via the virtual clinic.\n26. The method of claim 25, wherein the diagnostic center is portable.\n27. The method of claim 25, wherein the diagnostic center sells medical or medical-related products.\n28. The method of claim 25, wherein the administrator is not medically trained.\n29. The method of claim 25, wherein the user is an owner of a car, the particular one of the specialists is a car mechanic, and the test is performed on the user's car.\n30. The method of claim 25, wherein the user is an owner of an animal, the particular one of the specialists is a veterinarian, and the test is performed on the user's animal.\n31. A diagnostic center comprising:\na communication interface configured to establish real-time communication between an administrator who is not a licensed physician at the diagnostic center and a specialist via a virtual clinic, the diagnostic center located at a remote location away from a user's home and away from a dedicated treatment facility; and\na test interface operatively coupled to diagnostic testing equipment, the test interface configuring the diagnostic test equipment to perform a test;\nwherein the communication interface is operative to provide a test result to a specialist via the virtual clinic.\n32. The diagnostic center of claim 31, wherein the diagnostic center is portable.\n33. The diagnostic center of claim 31, wherein the diagnostic center sells medical or medical-related products.\n34. The diagnostic center of claim 31, wherein the administrator is not medically trained.\n35. The diagnostic center of claim 31, wherein the user is an owner of a car, the specialist is a car mechanic, and the test is performed on the user's car.\n36. The diagnostic center of claim 31, wherein the user is an owner of an animal, the specialist is a veterinarian, and the test is performed on the user's animal.\n37. A method comprising:\nusing a processor to establish real-time communication between a user and a specialist via a virtual clinic;\nproviding user information to the specialist via the virtual clinic;\nproviding instructions from the specialist to the user via the virtual clinic to perform a test using diagnostic test equipment operatively coupled to a diagnostic center at a remote location away from the user's home and away from a dedicated treatment facility, and the diagnostic center being administered by an administrator who is not a licensed physician;\nproviding a test result to the specialist via the virtual clinic; and\nreceiving and storing a diagnosis from the specialist.\n38. A method comprising:\nusing a processor to establish real-time communication between an administrator who is not a licensed physician at a diagnostic center and a specialist via a virtual clinic, the diagnostic center being at a remote location away from a user's home and away from a dedicated treatment facility;\nreceiving instructions to perform a test with diagnostic test equipment operatively coupled with the diagnostic center, the instructions received from the specialist; and\nproviding a test result from the diagnostic test equipment to the specialist via the virtual clinic.\n","abstract":""},{"grant_id":"US20190189259A1","inventor":"Gary Wayne Clark","claims":"What is claimed is:\n1. A method for generating an optimized treatment experience for a patient, the method comprising:\ncapturing patient experience data using at least one patient experience device, the patient experience data corresponding to a patient experience factor for the patient;\ndetermining a current level of the patient experience factor using a patient experience processing system, the current level of the patient experience factor determined based on the patient experience data;\ngenerating a customized therapy for the patient based on the current level of the patient experience factor, the customized therapy being an alternative treatment to a drug therapy administration; and\ngenerating an administration of a patient treatment experience based on the customized therapy, the patient treatment experience generated using the at least one patient experience device and including one or more of patient sense stimulation and patient cognitive stimulation.\n2. The method of claim 1, further comprising:\ncapturing patient drug data for the patient; and\ndetermining a current drug administration level for the patient using the patient experience processing system, the current drug administration level determined based on the patient drug data, the customized therapy being further generated based on the current drug administration level.\n3. The method of claim 1, further comprising:\ncapturing biometric response data using at least one patient monitoring device, the biometric response data captured during the administration of the patient experience treatment.\n4. The method of claim 3, further comprising:\ndetermining an effectiveness of the customized therapy in addressing the current level of the patient experience factor based on the biometric response data.\n5. The method of claim 4, further comprising:\nadjusting the patient experience treatment based on the effectiveness of the customized therapy.\n6. The method of claim 5, wherein the patient experience treatment is adjusted until the effectiveness includes a target change in the patient experience factor.\n7. The method of claim 6, the patient experience treatment includes playing at least one song, a beats per minute of the at least one song being automatically adjusted based on the biometric response data of the patient until the target change is reached.\n8. The method of claim 7, wherein the target change includes reaching a target biometric value.\n9. The method of claim 7, wherein the biometric response data includes a heart rate of the patient and the target biometric value includes a target heart rate, the beats per minute of the at least one song being adjusted until the heart rate of the patient meets the target heart rate.\n10. The method of claim 5, wherein the patient experience treatment is adjusted automatically.\n11. The method of claim 4, further comprising:\ngenerating processed therapy data by ingesting one or more of the biometric response data, the customized therapy, the effectiveness of the customized therapy, the patient experience data, and patient drug data.\n12. The method of claim 11, further comprising:\ngenerating a baseline for the patient experience factor and a patient profile for the patient using the processed therapy data.\n13. The method of claim 12, further comprising:\ngenerating a demographic profile based on the patient profile; and\ngenerating a demographic therapy for the demographic profile using the baseline for the patient experience factor.\n14. The method of claim 13, further comprising:\nselecting the demographic therapy for a second patient from a plurality of demographic therapies based on a match between the demographic profile with a second patient profile generated for the second patient.\n15. The method of claim 14, wherein the demographic therapy is customized for the second patient.\n16. The method of claim 4, further comprising:\ngenerating a ranking of the effectiveness of the customized therapy among a plurality of customized therapies for the patient experience factor, the ranking utilized to generate a patient management program for the patient, the patient management program including an aggregated set of the plurality of customized therapies.\n17. The method of claim 3, wherein the at least one patient monitoring device includes a biosensor configured to detect one or more biochemicals in saliva of the patient.\n18. The method of claim 1, further comprising:\npredicting a patient episode based on the current level of the patient experience factor.\n19. The method of claim 1, wherein generating the customized therapy includes generating a non-drug therapeutic index for the patient.\n20. The method of claim 1, wherein the sense stimulation includes at least one of sight stimulation, taste stimulation, hearing stimulation, touch stimulation, or smell stimulation.\n21. The method of claim 1, wherein the customized therapy is further based on one or more historical levels for the patient experience factor.\n22. The method of claim 1, wherein the customized therapy includes sound therapy with one or more patient curated playlists.\n23. The method of claim 1, wherein the patient experience factor is at least one of pain, stress, anxiety, depression, sleep, or mobility.\n24. The method of claim 1, wherein the patient experience data is captured through at least one of: self-reporting by the patient, reporting by a medical professional, facial recognition, gesture recognition, analysis of patient records, or biometric readings.\n25. The method of claim 1, wherein the customized therapy is combined with the drug therapy administration for an integrated treatment customized for the patient.\n","abstract":""},{"grant_id":"US11044364B2","inventor":"Mark Edward Gray","claims":"What is claimed is:\n1. A system for providing help, the system comprising:\na preprogrammed kit for deployment to premise, the preprogrammed kit comprising:\na digital assistant, the digital assistant is preprogrammed with account information and a skill for recognizing a preprogrammed specific utterance;\na virtual private network repeater, the virtual private network repeater having a wired connection for connecting to an existing modem and a wireless transceiver, whereas communications between the wireless transceiver and the digital assistant are preprogrammed including addresses and passwords;\nthe system further includes a plurality of agent computers, each of the plurality of agent computers connected to a server by a data network;\nwhereas the digital assistant is further preprogrammed to transmit a request for help through the virtual private network repeater to the server upon the digital assistant recognizing the specific utterance; and\nwhereas after receiving the request for the help, the server assigns one of the agent computers and forwards the request for the help to the one of the agent computers.\n2. The system of claim 1, wherein a generated user email address is generated and assigned to the digital assistant, identifying an account associated with the digital assistant.\n3. The system of claim 2, wherein a user account is created with a provider of the digital assistant using the generated user email address, thereby shielding from disclosure identifying information to the provider of the digital assistant.\n4. The system of claim 1, wherein the digital assistant automatically connects to the server through the virtual private network repeater.\n5. The system of claim 4, wherein the server periodically confirms that the digital assistant remains connected to the server and if the digital assistant is found not to be connected to the server, the server initiates an escalation.\n6. The system of claim 5, wherein if the digital assistant is found to be connected to the server, the server initiates a vocal request to the digital assistant and if the digital assistants does not receive a verbal answer to the vocal request within a predetermined time period, the server initiates the escalation.\n7. A method of providing help, the method comprising:\ngenerating a unique user email address;\ncreating a user account and assigning the unique user email address to the user account;\ncreating a provider account with a digital assistant provider using the unique user email;\nproviding a preprogrammed kit comprising a virtual private network repeater and at least one digital assistant, each digital assistant preprogrammed with an address of the virtual private network repeater, a password for accessing the virtual private network repeater, and the user account, preprogramming a skill into each of the digital assistant(s) for recognizing a specific utterance;\nproviding a connectivity between each of the digital assistant(s) and a server through the virtual private network repeater;\nlistening for the specific utterance by each of the digital assistant(s) and, upon recognizing the specific utterance by any of the digital assistant(s), the digital assistant sending a request for help to the server through the virtual private network repeater; and\nupon receiving the request for the help, the server forwarding the request for help to an agent computer.\n8. The method of claim 7, wherein the connectivity between each of the digital assistant and the server includes a mobile hotspot.\n9. The method of claim 7, wherein a mobile hotspot directly connects to the virtual private network repeater.\n10. The method of claim 7, further comprising the steps of:\nupon initialization, each of the digital assistant(s) automatically connects to the server.\n11. The method of claim 10, further comprising the steps of:\nperiodically determining if any of the digital assistant(s) is connected to the server and if any of the digital assistant(s) is disconnected from the server, initiating an escalation.\n12. The method of claim 11, further comprising the steps of:\nfor each of the digital assistant(s) that are connected to the server, sending a vocal request to each of the digital assistant(s) that are connected to the server and waiting for a predetermined time period for a vocal answer; if the vocal answer is not detected from at least one of the digital assistant(s) that are connected to the server within the predetermined time period, initiating the escalation.\n13. A system for providing help, the system comprising:\na unique user phone number generated for a user of the system for providing help for protecting privacy of the user;\na provider account created using the unique user phone number to protect the privacy of the user;\nan account for the user created in the system for providing help, the account having information regarding the user and the account having the unique user phone number;\na preprogrammed kit comprising a virtual private network repeater for connecting to an existing modem and a digital assistant; the digital assistant is preprogrammed with a skill recognizing a specific utterance and the digital assistant is pre-configured to connect with the virtual private network repeater;\nwhereas after the specific utterance is detected by the digital assistant, the digital assistant initiates a request for help and a call is made to a server, the call having a caller-id of the unique user phone number; the server answers the call, recognizes the unique user phone number, assigns an agent computer, and forwards the request for help to the agent computer.\n14. The system of claim 13, wherein the digital assistant automatically connects to the server through the virtual private network repeater.\n15. The system of claim 13, wherein the server periodically confirms that the digital assistant remains connected to the server and if the digital assistant is found not to be connected to the server, the server initiates an escalation.\n16. The system of claim 15, wherein if the digital assistant is found to be connected to the server, the server initiates a vocal request to the digital assistants and if the digital assistants does not receive a verbal answer to the vocal request within a predetermined time period, the server initiates the escalation.\n17. The system of claim 13, wherein the account in the system for providing help is indexed by the unique user phone number and the account includes an address of the user, health of the user, medications prescribed for the user, and contacts for local help related to the user.\n18. The system of claim 13, wherein the contacts for local help comprise a one or more contacts selected from the group consisting of a contact of a relative, the contact for local emergency medical services, a contact for local police, a contact of a clergy, a contact of a local fire department, and a contact of staff assigned to the user.\n","abstract":"A system for providing help includes a preprogrammed kit that includes at least one digital assistant and a virtual private network repeater for connecting to a data provider for connecting the digital assistant to a server. A plurality of agent computers is connected to the server by a data network. The digital assistant is preprogrammed with a skill for recognizing a preprogrammed specific utterance and the digital assistant is pre-configured to connect with the virtual private network repeater. After the preprogrammed specific utterance is detected by the digital assistant, the digital assistant initiates a request for help to the server and upon receiving the request for the help, the server assigns one of the agent computers and forwards the request for help to the one of the agent computers."},{"grant_id":"US11029918B2","inventor":"Fred A. BrownMitchell G. LawrenceVictor O'Brien Morrison","claims":"What is claimed is:\n1. One or more non-transitory computer-readable media storing computer-executable instructions that, when executed on one or more processors, cause the one or more processors to perform acts comprising:\ncausing display of a conversation user interface in conjunction with a content from a healthcare entity;\nreceiving patient input from a patient while the patient consumes the content from the healthcare entity, the input comprising one of verbal input, keypad input, or touch input;\nrepresenting the patient input in the conversation user interface;\ndetermining a context from the patient input;\ndetermining a first response to the user input based at least in part on medical records of the patient and assumptions derived from the context of the patient input;\nrepresenting the first response in the conversation user interface as a first message from a virtual assistant;\ndisplaying the assumptions derived from the context of the input in the conversational user input, wherein the assumptions are displayed separate from the first message;\nreceiving a modification to at least one of the displayed assumptions; and\nin response to receiving the modification:\ndetermining a second response to the patient input based at least in part on the medical records of the patient and the modified assumptions; and\nrefreshing the conversation GUI to replace the first message with a second message from the virtual assistant representing the second response.\n2. One or more non-transitory computer-readable media as recited in claim 1, wherein the input is verbal input, and the verbal input is represented in the conversation user interface in a text format through a speech recognition engine.\n3. One or more non-transitory computer-readable media as recited in claim 1, wherein the patient input is verbal input, and the determining the first response comprises a combination of interpreting the verbal input through speech recognition techniques and ascertaining a suitable response for the verbal input through natural language processing techniques.\n4. One or more non-transitory computer-readable media as recited in claim 3, wherein the assumptions used to determine the first response comprise first assumptions pertaining to speech recognition of the verbal input and second assumptions pertaining to natural language processing of the verbal input.\n5. One or more non-transitory computer-readable media as recited in claim 1, wherein the patient input is verbal input, and the acts further comprise:\nauthenticating an identity of the patient through analysis of the voice patterns contained in the verbal input.\n6. One or more non-transitory computer-readable media as recited in claim 1, wherein the context comprises past behavior of the patient and the assumptions are derived at least in part from the past behavior of the patient.\n7. One or more non-transitory computer-readable media as recited in claim 1, wherein the context comprises a status of the patient as checked in or checked out of a healthcare facility.\n8. One or more non-transitory computer-readable media as recited in claim 1, wherein the acts further comprise:\nreceiving a request from the patient to expose the assumptions used in determining the response; and\ndisplaying the assumptions derived from the content of the input in the conversational user input-in response to the request.\n9. One or more non-transitory computer-readable media as recited in claim 1, wherein the user input is verbal input, and the acts further comprise:\ndetermining that the patient input includes a query that cannot be processed unambiguously;\nin response to the determination, displaying a dialog representation indicating an ambiguity in the conversational user interface, wherein the dialog representation comprises a plurality of options;\nreceiving a selection of one of the options of the plurality of options;\nin response to the selection, determining a second response to the patient input based at least in part on the medical records of the patient and the selected option; and\nrepresenting the second response in the conversation user interface as a second message from the virtual assistant.\n10. One or more non-transitory computer-readable media as recited in claim 1, wherein the user input is verbal input, and the acts further comprise:\nwhen the patent input is verbal input, detecting that the patient is in pain based on one or more speech patterns of the patient input; and\ndetermining the context from the patient input and the detection that the patient is in pain.\n11. A system comprising:\none or more processors; and\nmemory storing computer-executable instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising:\ncausing display of a conversation user interface along with a visual representation of a virtual assistant, the virtual assistant being configured with a persona;\nreceiving patient input comprising at least one of verbal input, keypad input, or touch input;\nrepresenting the patient input in the conversation user interface;\ndetermining a context from the patient input;\ndetermining a first response to the patient input based at least in part on a medical record and an assumption derived from the context of the patient input;\nrepresenting the first response in the conversation user interface as a first message from the virtual assistant;\ndisplaying the assumption derived from the context of the patient input in the conversational user input, wherein the assumptions are displayed separate from the first message;\nreceiving a modification to the displayed assumption; and\nin response to receiving the modification:\ndetermining a second response to the patient input based at least in part on the medical records of the patient and the modified assumption; and\nrefreshing the conversation GUI to replace the first message with a second message from the virtual assistant representing the second response.\n12. The system of claim 11, wherein the determining the first response to the patient input is based on at least one of medical research results, nutrition information, or insurance information.\n13. The system of claim 11, wherein the operations further comprise:\nparsing the patient input to identify one or more concepts expressed therein; and\ndetermining an intent of the patient input based at least in part on the one or more concepts and the context; and\nwherein the determining the response to the patient input is based at least in part on the intent and the context.\n14. The system of claim 13, wherein the identifying the one or more concepts comprises identifying one or more keywords within the patient input and mapping the one or more keywords to the one or more concepts.\n15. The system of claim 13, wherein the determining of the intent comprises mapping the one or more concepts to one of multiple different intents associated with the one or more concepts based at least in part on the context.\n16. A method comprising:\ncausing, by a computing device, display of a conversation user interface along with a visual representation of a virtual assistant, the virtual assistant being configured with a persona;\nreceiving, by the computing device, patient input comprising at least one of verbal input, keypad input, or touch input;\nrepresenting, by the computing device, the patient input in the conversation user interface;\ndetermining, by the computing device, a context from the patient input;\ndetermining, by the computing device, a first response to the patient input based at least in part on a medical record of a patient and an assumption derived from the context of the patient input;\nrepresenting, by the computing device, the first response in the conversation user interface as a first message from the virtual assistant;\ndisplaying, by the computing device, the assumption derived from the context of the patient input in the conversational user input, wherein the assumptions are displayed separate from the first message;\nreceiving, by the computing device, a modification to the displayed assumption; and\nin response to receiving the modification:\ndetermining, by the computing device, a second response to the patient input based at least in part on the medical records of the patient and the modified assumption; and\nrefreshing, by the computing device, the conversation GUI to replace the first message with a second message from the virtual assistant representing.\n17. The method of claim 16, further comprising:\ndetermining an intent of the patient input based at least in part on one or more concepts and a first portion of the context; and\nwherein the determining the first response to the patient input is based at least in part on the intent and a second portion of the context.\n18. The method of claim 16, wherein the context comprises:\nwhether or not the patient has checked in with a healthcare entity;\na status of a medical device used by the patient;\na level of insurance coverage available to the patient;\na prescription medication used by the patient;\na diagnosed condition of the patient;\na time of day at which the patient provides the patient input to the virtual assistant;\na date on which the patient provides the patient input to the virtual assistant;\nan age of the patient;\na location of the patient; or\na device type of a device associated with the patient.\n19. The method of claim 16, further comprising:\ncausing an action on behalf of the patient, the action comprising at least one of:\nscheduling a medical appointment on behalf of the patient;\ninitiating a prescription refill request on behalf of the patient;\naccessing data from a medical device on behalf of the patient;\ncommunicating with an insurance provider on behalf of the patient; or\naltering a previously initiated action on behalf of the patient.\n20. The method of claim 16, further comprising:\nlearning over time that certain words of the patient input map to certain vocabulary terms or concepts.\n","abstract":"A conversation user interface enables a patient to better understand their healthcare by integrating diagnosis, treatment, medication management, and/or payment, through a system that uses a virtual assistant to engage in conversation with the patient. The conversation user interface conveys a visual representation of a conversation between the virtual assistant and the patient. An identity of the patient, including preferences and/or medical records, is maintained throughout interactions so that each aspect of this integrated system has access to the same information. The conversation user interface allows the patient to interact with the virtual assistant using natural language commands to receive information and complete a task related to his or her healthcare."},{"grant_id":"US20180082480A1","inventor":"John R. WhiteSeth Anderson NashAnnelise GallowayJody L. ClaypoolJ. Scott Owens","claims":"What is claimed is:\n1. An augmented reality device for use during a surgical procedure comprising:\nan augmented reality display to:\npresent, within a surgical field while permitting the surgical field to be viewed through the augmented reality display, a virtual indication of a surgical instrument present within the surgical field;\na processor to:\ndetermine whether the surgical instrument has been selected; and\ncause the virtual indication to be removed from display in response to determining that the surgical instrument has been selected.\n2. The augmented reality device of claim 1, further comprising a detection device to identify the surgical instrument from at least one of: a plurality of surgical instruments, a box including the surgical instrument, and a tray including the surgical instrument.\n3. The augmented reality device of claim 2, wherein the detection device includes at least one of a camera, an optical imaging device, and an electronic identification device.\n4. The augmented reality device of claim 1, further comprising a proximity detector, a motion sensor, or a button on the surgical instrument to identify the surgical instrument.\n5. The augmented reality device of claim 1, wherein the augmented reality device is to notify a second augmented reality device to instruct a user to obtain the surgical instrument or to alert a user that the surgical instrument has been obtained.\n6. The augmented reality device of claim 1, wherein the augmented reality display is further to display at least a portion of a view from a second augmented reality device.\n7. The augmented reality device of claim 1, wherein the augmented reality display is to include patient information in a heads-up portion of the augmented reality display.\n8. The augmented reality device of claim 1, wherein the augmented reality display is to present a series of virtual components to provide instruction for assembly of the surgical instrument.\n9. The augmented reality device of claim 8, wherein the series of virtual components are displayed in progression automatically in response to the processor detecting, automatically using a detection device or based on a user input, that an operation corresponding to one of the series of virtual components is complete.\n10. An augmented reality device for use in a surgical field comprising:\nan augmented reality display; and\na processor to:\nidentify a surgical procedure including a plurality of surgical steps;\ncause the augmented reality display to present, overlaid on a view of the surgical field while permitting the surgical field to be viewed through the augmented reality display, a virtual representation of a surgical step of the plurality of surgical steps;\ndetermine whether the surgical step has been completed; and\nin response to determining that the surgical step has been completed, cause the augmented reality to present, within the surgical field, a virtual representation of a second surgical step of the plurality of surgical steps.\n11. The augmented reality device of claim 10, wherein to identify the surgical procedure, the processor is to receive an indication of the surgical procedure via a user input using the augmented reality device.\n12. The augmented reality device of claim 10, wherein to identify the surgical procedure, the processor is to query a database to determine the surgical procedure based on at least one of a patient name or a patient identifier.\n13. The augmented reality device of claim 10, wherein to identify the surgical procedure, the processor is to determine whether the plurality of surgical steps are virtually representable; and\nin accordance with a determination that at least a portion of the plurality of surgical steps are virtually representable, the processor is to present the surgical step; and\nin accordance with a determination that a current step of the plurality of surgical steps is not virtually representable, the processor is to present an indication identifying the current step that is not virtually representable using the augmented reality display.\n14. The augmented reality device of claim 10, wherein the processor is further to determine that the current step that is not virtually representable has been completed, and in response, displaying the virtual representation of the second surgical step, the second surgical step being virtually representable within the augmented reality display.\n15. The augmented reality device of claim 10, wherein to present the surgical step, the processor is to present a virtual animation of the surgical step.\n16. The augmented reality device of claim 10, wherein to present the surgical step, the processor is to present a previously captured video of the surgical step.\n17. The augmented reality device of claim 10, wherein the processor is further to receive a surgeon control command to cause an aspect of the virtual representation of the surgical step to rotate, zoom, translate, play, rewind, fast forward, pause, restart, or end.\n18. The augmented reality device of claim 10, wherein to determine that the surgical step has been completed, the processor is to receive an indication that the surgical step has been completed via a user input using the augmented reality device.\n19. The augmented reality device of claim 10, wherein to determine that the surgical step has been completed, the processor is to identify a gesture using a camera of the augmented reality device.\n20. The augmented reality device of claim 10, wherein to determine that the surgical step has been completed, the processor is to identify an aspect of the surgical step using a camera of the augmented reality device, and based on the identified aspect, determine that the surgical step has been completed.\n21. The augmented reality device of claim 10, wherein the processor is further to determine whether the second surgical step has been completed;\nin response to determining that the second surgical step has been completed, the processor is to determine whether there are any remaining surgical steps in the plurality of surgical steps that have not yet been presented;\nin accordance with a determination that there is at least one surgical step remaining, the processor is to present a virtual representation of the at least one surgical step;\nin accordance with a determination that there are no remaining surgical steps, the processor is to present a virtual indication that the surgical procedure is complete.\n22. A method for using an augmented reality device in a surgical field comprising:\nidentifying, using a processor, a surgical procedure including a plurality of surgical steps;\npresenting using an augmented reality display of the augmented reality device, overlaid on a view of the surgical field while permitting the surgical field to viewed through the augmented reality display, a virtual representation of a first surgical step of the plurality of surgical steps;\ndetermining whether the first surgical step has been completed; and\nin response to determining that the surgical step has been completed, presenting using an augmented reality display of the augmented reality device, within the surgical field, a virtual representation of a second surgical step of the plurality of surgical steps.\n23. The method of claim 22, further comprising displaying patient information in a heads-up display portion of the augmented reality display.\n24. The method of claim 22, further comprising:\nbefore determining that the surgical step has been completed, determining that the surgical step is not being performed;\ndetermining that a different surgical step of the plurality of surgical steps is being performed;\ndisplaying an alert using the augmented reality display indicating that the different surgical step is being performed; and\nproviding a user selectable option to switch to presenting a virtual representation of the different surgical step.\n25. The method of claim 22, further comprising presenting a virtual guidance marker on a patient corresponding to an aspect of the surgical step, the virtual guidance marker indicating a location to perform the surgical step, including at least one of an incision location, a resection location, an implant location, a force direction, or a pin location.\n26. The method of claim 22, wherein the surgical steps are ordered and the surgical step comes before the second surgical step in the ordered surgical steps, and wherein the virtual representation of the surgical step is presented before the virtual representation of the second surgical step.\n27. A method for using an augmented reality device in a surgical field comprising:\nidentifying, using a processor, a surgical procedure including a plurality of surgical steps;\ndetermining, from the surgical procedure, a surgical instrument needed for a surgical step of the plurality of surgical steps;\npresenting using an augmented reality display of the augmented reality device, overlaid on a view of the surgical field while permitting the surgical field to viewed through the augmented reality display, a virtual representation of the surgical instrument;\ndetermining whether the surgical step has been completed; and\nin response to determining that the surgical step has been completed, presenting using an augmented reality display of the augmented reality device, within the surgical field, a virtual representation of a second instrument for a second surgical step of the plurality of surgical steps.\n28. The method of claim 27, further comprising identifying the second surgical step based on identifying a visual or audio indication.\n29. The method of claim 27, further comprising, determining whether the surgical instrument is needed in a later surgical step of the plurality of surgical steps.\n30. The method of claim 29, further comprising, in response to determining that the surgical instrument is needed in the later surgical step, displaying a virtual indication that the surgical instrument is needed in the later surgical step.\n","abstract":"A system and method for using augmented reality device for use during a surgical procedure are described. A system may include an augmented reality device to present a virtual indication, such as a virtual indication of a surgical instrument, a force vector, a direction, or the like. The augmented reality device may present a virtual aspect of a procedure, such as a virtual animation of a step of a procedure. The augmented reality device may present a virtual object, indication, aspect, etc., within a surgical field while permitting the surgical field or aspects of the surgical field to be viewed through the augmented reality display (e.g., presenting virtual objects mixed with real objects)."},{"grant_id":"US11544953B2","inventor":"Michael Kusens","claims":"What is claimed is:\n1. A computerized method of monitoring an individual, the computerized method being performed by a computerized monitoring system and comprising:\nreceiving, from one or more 3D motion sensors, image data of a room to be monitored;\nconfiguring a virtual barrier within the room;\nutilizing skeletal tracking to initially track a position of a person detected within the room to determine whether the person touches or crosses the virtual barrier; and\nswitching from skeletal tracking to bounding box tracking to track the position of the person to determine whether the person touches or crosses the virtual barrier, wherein bounding box tracking includes monitoring a position of a bounding box circumscribing at least a portion of the person relative to the virtual barrier.\n2. The computerized method of claim 1, wherein switching from skeletal tracking to bounding box tracking occurs upon determining that no crossings of the virtual barrier have been detected for the person within a threshold period of time.\n3. The computerized method of claim 1, wherein switching from skeletal tracking to bounding box tracking occurs upon detecting one or more of low-lighting conditions and an object obstructing skeletal tracking from the image data.\n4. The computerized method of claim 1, wherein the bounding box is configured to circumscribe an entire body of the person.\n5. The computerized method of claim 1 further comprising sending an alert to a computerized communication system upon detecting that the bounding box touches or crosses the virtual barrier.\n6. The computerized method of claim 5, wherein, upon receiving the alert, the computerized communication system notifies the person, a caregiver, or clinician that the virtual barrier has been crossed.\n7. The computerized method of claim 1 further comprising determining that a minimum portion of the bounding box touches or crosses the virtual barrier.\n8. The computerized method of claim 1 further comprising determining the identity of the person.\n9. A system for monitoring an individual, the system comprising:\na computerized monitoring system in communication with one or more 3D motion sensors; and\na computerized communication system;\nwherein the computerized monitoring system is configured to:\nreceive, from one or more 3D motion sensors, image data of a room to be monitored;\nconfigure a virtual barrier within the room;\nutilize skeletal tracking to initially track a position of a person detected within the room to determine whether the person touches or crosses the virtual barrier; and\nswitch from skeletal tracking to bounding box tracking to track the position of the person to determine whether the person touches or crosses the virtual barrier, wherein bounding box tracking includes monitoring a position of a bounding box circumscribing at least a portion of the person relative to the virtual barrier.\n10. The system of claim 9, wherein switching from skeletal tracking to bounding box tracking occurs upon determining that no crossings of the virtual barrier have been detected for the person within a threshold period of time.\n11. The system of claim 9, wherein switching from skeletal tracking to bounding box tracking occurs upon detecting one or more of low-lighting conditions and an object obstructing skeletal tracking from the image data.\n12. The system of claim 9, wherein the bounding box is configured to circumscribe an entire body of the person.\n13. The system of claim 9, wherein the computerized monitoring system is configured to send an alert to the computerized communication system upon detecting that the bounding box touches or crosses the virtual barrier.\n14. The system of claim 13, wherein the computerized communication system is configured to, upon receiving the alert from the computerized monitoring system, notify the person, a caregiver, or clinician that the virtual barrier has been crossed.\n15. Non-transitory computer-readable media having embodied thereon instructions that, when executed by a computer processor, cause the computer processor to:\nreceive, from one or more 3D motion sensors, image data of a room to be monitored;\nconfigure a virtual barrier within the room;\nutilize skeletal tracking to initially track a position of a person detected within the room to determine whether the person touches or crosses the virtual barrier; and\nswitch from skeletal tracking to bounding box tracking to track the position of the person to determine whether the person touches or crosses the virtual barrier, wherein bounding box tracking includes monitoring a position of a bounding box circumscribing at least a portion of the person relative to the virtual barrier.\n16. The non-transitory computer-readable media of claim 15, wherein switching from skeletal tracking to bounding box tracking occurs upon determining that no crossings of the virtual barrier have been detected for the person within a threshold period of time.\n17. The non-transitory computer-readable media of claim 15, wherein switching from skeletal tracking to bounding box tracking occurs upon detecting one or more of low-lighting conditions and an object obstructing skeletal tracking from the image data.\n18. The non-transitory computer-readable media of claim 15, wherein the bounding box is configured to circumscribe an entire body of the person.\n19. The non-transitory computer-readable media of claim 15, wherein the instructions further cause the computer processor to send an alert to a computerized communication system upon detecting that the bounding box touches or crosses the virtual barrier.\n20. The non-transitory computer-readable media of claim 19, wherein the instructions further cause the computer processor to send another alert to the computerized communication system upon detecting that all of the virtual bounding box crosses the virtual barrier.\n","abstract":"Systems, methods and media are disclosed for identifying the crossing of a virtual barrier. A person in a 3D image of a room may be circumscribed by a bounding box. The position of the bounding box may be monitored over time, relative to the virtual barrier. If the bounding box touches or crosses the virtual barrier, an alert may be sent to the person being monitored, a caregiver or a clinician. Bounding box tracking may be used in addition to or instead of an initial tracking process, such as skeletal tracking."}]